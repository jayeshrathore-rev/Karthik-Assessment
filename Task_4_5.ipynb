{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up summary report with many-to-many mapping generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Emails, Text Messages, Voicemails, and Property Info\n",
    "emails = pd.read_csv('aggregated_email_data.csv')\n",
    "text_messages = pd.read_csv('updated_text_message_logs.csv')\n",
    "voicemails = pd.read_csv('updated_voicemail_logs.csv')\n",
    "\n",
    "# Load property info from the 'Property & Alias Info.xlsx' file\n",
    "property_info = pd.read_excel('Property & Alias Info.xlsx', sheet_name='PROPERTY INFO')\n",
    "property_info = property_info[['PID']]  # Assuming 'PID' is the column for Property ID\n",
    "property_info.rename(columns={'PID': 'Property ID'}, inplace=True)\n",
    "\n",
    "# Load alias info from the 'ALIAS INFO' sheet\n",
    "alias_info = pd.read_excel('Property & Alias Info.xlsx', sheet_name='ALIAS INFO')\n",
    " \n",
    "alias_info.rename(columns={'#': 'Alias ID'}, inplace=True)  # Renaming for clarity\n",
    "\n",
    "# Step 2: Normalize the columns in each communication data type\n",
    "# Emails: Relevant columns: 'Property ID', 'Alias ID', 'Date Time', 'Body'\n",
    "emails = emails[['Property ID', 'Alias ID', 'Date Time', 'Body', 'Attachments']]\n",
    "emails.rename(columns={'Date Time': 'Date', 'Body': 'Summary of content'}, inplace=True)\n",
    "emails['Type of communication'] = 'email'\n",
    "\n",
    "# Text messages: Relevant columns: 'Property ID', 'Alias ID', 'date', 'encrypted_aes_text'\n",
    "text_messages = text_messages[['Property ID', 'Alias ID', 'date', 'encrypted_aes_text']]\n",
    "text_messages.rename(columns={'encrypted_aes_text': 'Summary of content', 'date': 'Date'}, inplace=True)\n",
    "text_messages['Type of communication'] = 'text'\n",
    "\n",
    "# Voicemails: Relevant columns: 'Property ID', 'Alias ID', 'date', 'transcription_text'\n",
    "voicemails = voicemails[['Property ID', 'Alias ID', 'date', 'transcription_text']]\n",
    "voicemails.rename(columns={'transcription_text': 'Summary of content', 'date': 'Date'}, inplace=True)\n",
    "voicemails['Type of communication'] = 'voicemail'\n",
    "\n",
    "text_messages['Attachments'] = pd.NA  # Assigning NaN for non-email types\n",
    "voicemails['Attachments'] = pd.NA\n",
    "\n",
    "# Step 3: Combine all follow-up data into one DataFrame (Merged Data)\n",
    "merged_follow_ups = pd.concat([emails, text_messages, voicemails], ignore_index=True)\n",
    "\n",
    "merged_follow_ups.dropna(subset=['Property ID', 'Alias ID'], inplace=True)\n",
    "\n",
    "# Add a new column for the primary key (auto-incrementing ID\n",
    "merged_follow_ups['ID'] = range(1 , len(merged_follow_ups)+ 1 )\n",
    "\n",
    "merged_follow_ups.to_csv('Merged_Follow_Up_Data.csv', index=False)\n",
    "\n",
    "# Step 4: Generate a full Cartesian product (many-to-many) of all Property IDs and Alias IDs\n",
    "property_alias_combinations = pd.merge(\n",
    "    pd.DataFrame({'Property ID': property_info['Property ID'].unique()}),\n",
    "    pd.DataFrame({'Alias ID': alias_info['Alias ID'].unique()}),\n",
    "    how='cross'  # This creates the full combination of each Property ID with each Alias ID\n",
    ")\n",
    "\n",
    "# Step 5: Merge the follow-up data with the full property-alias combinations\n",
    "merged_follow_ups_full = pd.merge(\n",
    "    property_alias_combinations,\n",
    "    merged_follow_ups,\n",
    "    how='left',  # Use left join to keep all property-alias combinations, even if no follow-ups exist\n",
    "    on=['Property ID', 'Alias ID']\n",
    ")\n",
    "\n",
    "# Step 6: Count the number of follow-ups for each property-alias combination\n",
    "# This filters rows where 'Summary of content' is not null\n",
    "follow_up_counts = merged_follow_ups_full.groupby(['Property ID', 'Alias ID'])['Summary of content'].count().reset_index(name='Total Follow-Ups')\n",
    "\n",
    "# Step 7: Ensure combinations with no follow-ups are filled with 'No Follow up'\n",
    "follow_up_counts['Total Follow-Ups'] = follow_up_counts['Total Follow-Ups'].replace(0, pd.NA).fillna('No Follow up')\n",
    "\n",
    "\n",
    "# Step 5: Load Titanium Fups and Scandium Fups\n",
    "titanium_fups = pd.read_excel('Titanium Fups.xlsx')[['Alias ID', 'Property ID']]\n",
    "scandium_fups = pd.read_excel('Scandium Fups.xlsx')[['Alias ID', 'Property ID']]\n",
    "\n",
    "# Combine the Fups data and remove duplicates\n",
    "combined_fups = pd.concat([titanium_fups, scandium_fups]).drop_duplicates()\n",
    "\n",
    "# Step 6: Use `isin()` to Find Matching Rows\n",
    "# Create a combined key for comparison\n",
    "follow_up_counts['combined_key'] = follow_up_counts['Property ID'].astype(str) + '-' + follow_up_counts['Alias ID'].astype(str)\n",
    "combined_fups['combined_key'] = combined_fups['Property ID'].astype(str) + '-' + combined_fups['Alias ID'].astype(str)\n",
    "\n",
    "# Set \"Not Contacted\" where there is no match in the combined Fups data\n",
    "follow_up_counts['Total Follow-Ups'] = follow_up_counts.apply(\n",
    "    lambda row: 'Not Contacted' if row['combined_key'] not in combined_fups['combined_key'].values else row['Total Follow-Ups'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the combined key column\n",
    "follow_up_counts.drop(columns=['combined_key'], inplace=True)\n",
    "\n",
    "# Add a new column for the primary key (auto-incrementing ID\n",
    "follow_up_counts['ID'] = range(1 , len(follow_up_counts)+ 1 )\n",
    "\n",
    "# Step 8: Save the final follow-up summary report with all property-alias combinations\n",
    "follow_up_counts.to_csv('Follow_Up_Summary_Report.csv', index=False)\n",
    "\n",
    "print(\"Follow-up summary report with many-to-many mapping generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified follow-up data has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def contains_keywords(content, keywords):\n",
    "    # Create a regex pattern that combines all keywords, allowing for case-insensitive matches\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, keywords)) + r')\\b'\n",
    "    return bool(re.search(pattern, content, re.IGNORECASE))\n",
    "\n",
    "def classify_follow_up(row, alias_info):\n",
    "    content = row['Summary of content'].lower() if pd.notna(row['Summary of content']) else ''\n",
    "    attachments = row['Attachments'] if pd.notna(row['Attachments']) else ''\n",
    "\n",
    "    # Check for tour confirmation using regex\n",
    "    tour_confirmation_keywords = [\n",
    "    'tour confirmation',\n",
    "    'confirmed your tour',\n",
    "    'tour is confirmed',\n",
    "    'tour is booked',\n",
    "    'appointment tour',\n",
    "    'appointment is confirmed',\n",
    "    'tour reservation',\n",
    "    'tour has been confirmed',\n",
    "    'confirmation for your tour',\n",
    "    'confirmed your tour',\n",
    "    'your appointment'\n",
    "]\n",
    "    row['Tour Confirmation'] = 'Yes' if contains_keywords(content, tour_confirmation_keywords) else 'No'\n",
    "\n",
    "    # Check for Booking Link\n",
    "    booking_link_keywords = [\n",
    "    'booking',\n",
    "    'schedule',\n",
    "    'book',\n",
    "    'tour',\n",
    "    'reservation',\n",
    "    'reserve'\n",
    "    ]\n",
    "    booking_link_pattern = r'(http[s]?://\\S+)'  # Regex to identify URLs\n",
    "    row['Booking Link'] = 'Yes' if contains_keywords(content, booking_link_keywords) and re.search(booking_link_pattern, content) else 'No'\n",
    "\n",
    "    # Check for Requests Tour Booking\n",
    "    request_tour_keywords = [\n",
    "    'booking a tour',\n",
    "    'schedule a tour',\n",
    "    'book a tour'\n",
    "    'reserve a tour',\n",
    "    'request a tour',\n",
    "    'arrange a tour'\n",
    "]\n",
    "    row['Requests Tour Booking'] = 'Yes' if contains_keywords(content, request_tour_keywords) else 'No'\n",
    "\n",
    "    # Contains Pictures\n",
    "    picture_filetypes = ['.jpg', '.jpeg', '.png', '.gif' , '.webp']\n",
    "    row['Contains Pictures'] = 'Yes' if any(ext in attachments.lower() for ext in picture_filetypes) else 'No'\n",
    "\n",
    "    # Check for alias name\n",
    "    if pd.notna(row['Alias ID']):\n",
    "        alias_row = alias_info.loc[alias_info['Alias ID'] == row['Alias ID']]\n",
    "        \n",
    "        if not alias_row.empty:\n",
    "            alias_name = alias_row['ALIAS NAME'].values[0]\n",
    "            # Split the alias name into substrings and ignore common words like \"or\"\n",
    "            substrings = [word for word in alias_name.lower().split() if word not in ['or']]\n",
    "            \n",
    "            # Check if any substring is in the content\n",
    "            if any(substring in content.lower() for substring in substrings):\n",
    "                row['Personalized or Generalized'] = 'Personalized'\n",
    "            else:\n",
    "                row['Personalized or Generalized'] = 'Generalized'\n",
    "        else:\n",
    "            row['Personalized or Generalized'] = 'Generalized'\n",
    "    else:\n",
    "        row['Personalized or Generalized'] = 'Generalized'\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply the classification to the follow-up data\n",
    "classified_follow_ups = merged_follow_ups.apply(lambda row: classify_follow_up(row, alias_info), axis=1)\n",
    "\n",
    "# Save the classified follow-up data\n",
    "classified_follow_ups.to_csv('Classified_Follow_Up_Data.csv', index=False)\n",
    "\n",
    "print(\"Classified follow-up data has been generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
